{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ef1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 4: UMFASSENDE ERWEITERTE ANALYSEN ===\n",
      "Netzwerk-Topologie, Anomalie-Deep-Dive, Predictive Analytics & Qualitätsanalysen\n",
      "=====================================================================================\n",
      "=====================================================================================\n",
      "📋 ANWEISUNGEN FÜR PHASE 4:\n",
      "=====================================================================================\n",
      "1. Passen Sie die Dateipfade IPv4_FILE und IPv6_FILE an (Zeile ~970-971)\n",
      "2. Führen Sie run_comprehensive_analysis() aus\n",
      "3. Die Analyse dauert mehrere Minuten - seien Sie geduldig!\n",
      "4. Alle Ergebnisse werden in der Konsole ausgegeben\n",
      "=====================================================================================\n",
      "🔄 LADE DATEN FÜR UMFASSENDE ANALYSE...\n",
      "Versuche IPv4-Datei zu laden: ../data/IPv4.parquet\n",
      "Versuche IPv6-Datei zu laden: ../data/IPv6.parquet\n",
      "✅ IPv4: 160,923 Messungen erfolgreich geladen\n",
      "✅ IPv6: 160,923 Messungen erfolgreich geladen\n",
      "🚀 BEIDE DATEIEN ERFOLGREICH GELADEN - STARTE UMFASSENDE ANALYSE...\n",
      "\n",
      "=====================================================================================\n",
      "UMFASSENDE ANALYSE FÜR IPv4\n",
      "=====================================================================================\n",
      "🌐 Starte Netzwerk-Topologie-Analyse...\n",
      "\n",
      "1. NETZWERK-TOPOLOGIE & INFRASTRUKTUR - IPv4\n",
      "------------------------------------------------------------\n",
      "\n",
      "🌐 HOP-BY-HOP NETZWERK-ANALYSE:\n",
      "Netzwerk-Pfade analysiert: 160,923\n",
      "\n",
      "📊 ASN-DIVERSITÄT PRO SERVICE:\n",
      "  Heise:\n",
      "    Gesamte ASNs: 6\n",
      "    Durchschn. ASNs/Region: 3.5\n",
      "    Gemeinsame ASNs: 1 (16.7%)\n",
      "  Quad9 DNS:\n",
      "    Gesamte ASNs: 5\n",
      "    Durchschn. ASNs/Region: 2.2\n",
      "    Gemeinsame ASNs: 1 (20.0%)\n",
      "  Berkeley NTP:\n",
      "    Gesamte ASNs: 10\n",
      "    Durchschn. ASNs/Region: 5.1\n",
      "    Gemeinsame ASNs: 2 (20.0%)\n",
      "  Google DNS:\n",
      "    Gesamte ASNs: 2\n",
      "    Durchschn. ASNs/Region: 1.8\n",
      "    Gemeinsame ASNs: 1 (50.0%)\n",
      "  Akamai CDN:\n",
      "    Gesamte ASNs: 4\n",
      "    Durchschn. ASNs/Region: 2.9\n",
      "    Gemeinsame ASNs: 2 (50.0%)\n",
      "  Cloudflare DNS:\n",
      "    Gesamte ASNs: 8\n",
      "    Durchschn. ASNs/Region: 2.7\n",
      "    Gemeinsame ASNs: 1 (12.5%)\n",
      "  Cloudflare CDN:\n",
      "    Gesamte ASNs: 5\n",
      "    Durchschn. ASNs/Region: 2.5\n",
      "    Gemeinsame ASNs: 2 (40.0%)\n",
      "\n",
      "🏢 TRANSIT-PROVIDER-TIER-ANALYSE:\n",
      "  Hyperscaler: 3 ASNs (AS13335, AS15169, AS16509...)\n",
      "  Tier-1: 6 ASNs (AS1299, AS174, AS3257, AS3356, AS5511...)\n",
      "\n",
      "🔢 HOP-COUNT-ANALYSE:\n",
      "  UNICAST:\n",
      "    Durchschn. Hops: 16.9\n",
      "    Min-Max Hops: 5-27\n",
      "    Std.Dev.: 4.6\n",
      "  ANYCAST:\n",
      "    Durchschn. Hops: 7.6\n",
      "    Min-Max Hops: 2-18\n",
      "    Std.Dev.: 2.0\n",
      "  PSEUDO-ANYCAST:\n",
      "    Durchschn. Hops: 18.6\n",
      "    Min-Max Hops: 12-30\n",
      "    Std.Dev.: 3.5\n",
      "🚨 Starte Anomalie-Deep-Dive...\n",
      "\n",
      "2. ANOMALIE-DEEP-DIVE UND KLASSIFIKATION - IPv4\n",
      "------------------------------------------------------------\n",
      "Performance-Daten für Anomalie-Analyse: 160,923\n",
      "\n",
      "🚨 ERWEITERTE ANOMALIE-KLASSIFIKATION:\n",
      "Gesamte Anomalien: 24,290\n",
      "\n",
      "Anomalie-Typen:\n",
      "  latency_spike: 8,798 (36.2%)\n",
      "  jitter_spike: 7,980 (32.9%)\n",
      "  routing_change: 7,425 (30.6%)\n",
      "  packet_loss: 87 (0.4%)\n",
      "\n",
      "🌍 GEOGRAFISCHE ANOMALIE-HOTSPOTS:\n",
      "  af-south-1: 5,382 Anomalien (33.43% Rate)\n",
      "  eu-north-1: 4,702 Anomalien (29.20% Rate)\n",
      "  ap-east-1: 3,608 Anomalien (22.42% Rate)\n",
      "  us-west-1: 3,303 Anomalien (20.52% Rate)\n",
      "  ca-central-1: 2,204 Anomalien (13.68% Rate)\n",
      "  ap-southeast-2: 1,954 Anomalien (12.15% Rate)\n",
      "  sa-east-1: 1,133 Anomalien (7.04% Rate)\n",
      "  ap-south-1: 866 Anomalien (5.38% Rate)\n",
      "  ap-northeast-1: 601 Anomalien (3.74% Rate)\n",
      "  eu-central-1: 537 Anomalien (3.34% Rate)\n",
      "\n",
      "📊 ANOMALIE-VERHALTEN PRO SERVICE-TYP:\n",
      "type            jitter_spike  latency_spike  packet_loss  routing_change\n",
      "service_type                                                            \n",
      "anycast                 4547           8767           61              47\n",
      "pseudo-anycast          1138              3           10            3535\n",
      "unicast                 2295             28           16            3843\n",
      "\n",
      "⏰ TEMPORALE ANOMALIE-CLUSTER:\n",
      "Peak Anomalie-Stunden: {4: np.int64(1080), 6: np.int64(1079), 13: np.int64(1070)}\n",
      "\n",
      "🏢 PROVIDER-ANOMALIE-PROFILE:\n",
      "  Heise: 5.44% Anomalie-Rate\n",
      "    Schweregrade: {'medium': np.int64(1178), 'low': np.int64(67), 'high': np.int64(5)}\n",
      "  Quad9: 15.49% Anomalie-Rate\n",
      "    Schweregrade: {'high': np.int64(2315), 'medium': np.int64(1233), 'low': np.int64(13)}\n",
      "  UC Berkeley: 21.45% Anomalie-Rate\n",
      "    Schweregrade: {'low': np.int64(3776), 'medium': np.int64(1156)}\n",
      "  Google: 14.98% Anomalie-Rate\n",
      "    Schweregrade: {'high': np.int64(2303), 'medium': np.int64(1122), 'low': np.int64(18)}\n",
      "  Akamai: 20.38% Anomalie-Rate\n",
      "    Schweregrade: {'low': np.int64(3535), 'medium': np.int64(1151)}\n",
      "  Cloudflare: 13.96% Anomalie-Rate\n",
      "    Schweregrade: {'medium': np.int64(6308), 'high': np.int64(94), 'low': np.int64(16)}\n",
      "🏗️ Starte Provider-Infrastruktur-Mapping...\n",
      "\n",
      "3. PROVIDER-INFRASTRUKTUR-MAPPING - IPv4\n",
      "-------------------------------------------------------\n",
      "\n",
      "🏗️ PROVIDER-EDGE-INFRASTRUKTUR-ANALYSE:\n",
      "\n",
      "📍 EDGE-SERVER-DENSITY:\n",
      "\n",
      "  Cloudflare:\n",
      "    Gesamte Edge-Server: 20\n",
      "    Regionen abgedeckt: 10\n",
      "    Durchschn. Edges/Region: 2.0\n",
      "    Top-Regionen: {'eu-central-1': 2, 'sa-east-1': 2, 'af-south-1': 2}\n",
      "    Geografische Indikatoren: 7\n",
      "\n",
      "  Google:\n",
      "    Gesamte Edge-Server: 10\n",
      "    Regionen abgedeckt: 10\n",
      "    Durchschn. Edges/Region: 1.0\n",
      "    Top-Regionen: {'ca-central-1': 1, 'eu-north-1': 1, 'ap-south-1': 1}\n",
      "    Geografische Indikatoren: 2\n",
      "\n",
      "  Quad9:\n",
      "    Gesamte Edge-Server: 10\n",
      "    Regionen abgedeckt: 10\n",
      "    Durchschn. Edges/Region: 1.0\n",
      "    Top-Regionen: {'eu-north-1': 1, 'ap-south-1': 1, 'ap-northeast-1': 1}\n",
      "    Geografische Indikatoren: 12\n",
      "\n",
      "📊 PROVIDER-INFRASTRUKTUR-VERGLEICH:\n",
      "     Provider  Total_Edges  Regions  Avg_Edges_Per_Region  Geo_Coverage  \\\n",
      "0  Cloudflare           20       10                   2.0             7   \n",
      "1      Google           10       10                   1.0             2   \n",
      "2       Quad9           10       10                   1.0            12   \n",
      "\n",
      "   Anomaly_Rate_%  \n",
      "0           13.96  \n",
      "1           14.98  \n",
      "2           15.49  \n",
      "\n",
      "⭐ INFRASTRUKTUR-EFFIZIENZ-RANKING:\n",
      "  Cloudflare: 2.8/10\n",
      "    Edge-Density: 2.0/10\n",
      "    Geo-Coverage: 3.5/10\n",
      "    Stabilität: 3.0/10\n",
      "  Google: 1.5/10\n",
      "    Edge-Density: 1.0/10\n",
      "    Geo-Coverage: 1.0/10\n",
      "    Stabilität: 2.5/10\n",
      "  Quad9: 3.1/10\n",
      "    Edge-Density: 1.0/10\n",
      "    Geo-Coverage: 6.0/10\n",
      "    Stabilität: 2.3/10\n",
      "📊 Starte Statistische & Prädiktive Analysen...\n",
      "\n",
      "4. STATISTISCHE & PRÄDIKTIVE ANALYSEN - IPv4\n",
      "-------------------------------------------------------\n",
      "\n",
      "📈 KORRELATIONSANALYSE:\n",
      "Korrelations-Matrix:\n",
      "                       latency  packet_loss  jitter  intermediate_failures  \\\n",
      "latency                  1.000        0.027   0.057                  0.375   \n",
      "packet_loss              0.027        1.000   0.019                 -0.014   \n",
      "jitter                   0.057        0.019   1.000                  0.001   \n",
      "intermediate_failures    0.375       -0.014   0.001                  1.000   \n",
      "total_hops               0.801       -0.009   0.015                  0.639   \n",
      "\n",
      "                       total_hops  \n",
      "latency                     0.801  \n",
      "packet_loss                -0.009  \n",
      "jitter                      0.015  \n",
      "intermediate_failures       0.639  \n",
      "total_hops                  1.000  \n",
      "\n",
      "🔍 SIGNIFIKANTE KORRELATIONEN (|r| > 0.3):\n",
      "  latency ↔ intermediate_failures: 0.375\n",
      "  latency ↔ total_hops: 0.801\n",
      "  intermediate_failures ↔ total_hops: 0.639\n",
      "\n",
      "🕰️ TIME-SERIES-CLUSTERING:\n",
      "Provider-Performance-Cluster:\n",
      "  Cluster 0: ['Google']\n",
      "    Durchschn. Latenz: 3.69ms\n",
      "    Durchschn. Stabilität: 7.08ms\n",
      "  Cluster 1: ['Cloudflare']\n",
      "    Durchschn. Latenz: 1.82ms\n",
      "    Durchschn. Stabilität: 3.63ms\n",
      "  Cluster 2: ['Quad9']\n",
      "    Durchschn. Latenz: 2.85ms\n",
      "    Durchschn. Stabilität: 5.48ms\n",
      "\n",
      "🔮 PREDICTIVE MODELING:\n",
      "  Latenz-Vorhersage-Modell:\n",
      "    R² Score: 0.863\n",
      "    RMSE: 33.886ms\n",
      "    Feature-Wichtigkeit:\n",
      "      hour: 0.013\n",
      "      day_of_week_num: 0.009\n",
      "      total_hops: 0.315\n",
      "      service_type_numeric: 0.662\n",
      "\n",
      "  Performance-Kategorie-Vorhersage:\n",
      "    Latenz-Kategorien:\n",
      "      Excellent: 94,030 (58.4%)\n",
      "      Poor: 52,522 (32.6%)\n",
      "      Fair: 11,320 (7.0%)\n",
      "      Good: 3,051 (1.9%)\n",
      "📋 Starte Qualitäts- und SLA-Analysen...\n",
      "\n",
      "5. QUALITÄTS- UND SLA-ANALYSEN - IPv4\n",
      "---------------------------------------------\n",
      "\n",
      "📊 SLA-COMPLIANCE-ANALYSE:\n",
      "SLA-Schwellwerte:\n",
      "  latency_excellent: 5\n",
      "  latency_good: 20\n",
      "  latency_acceptable: 100\n",
      "  packet_loss_max: 1\n",
      "  availability_min: 99.9\n",
      "\n",
      "SLA-Compliance-Übersicht:\n",
      "     Service_Type  Excellent_Latency_%  Good_Latency_%  Acceptable_Latency_%  \\\n",
      "0         unicast                  9.9            10.0                  24.9   \n",
      "1         anycast                 94.8            98.1                 100.0   \n",
      "2  pseudo-anycast                 10.0            10.0                  21.8   \n",
      "\n",
      "   Low_PacketLoss_%  Availability_%  \n",
      "0              99.7           100.0  \n",
      "1              99.9           100.0  \n",
      "2              99.9           100.0  \n",
      "\n",
      "🏢 PROVIDER-SLA-SCORECARD:\n",
      "\n",
      "  Quad9:\n",
      "    Excellent Performance: 89.8%\n",
      "    Availability: 100.0%\n",
      "    Reliability: 99.5%\n",
      "    95th Percentile Latenz: 13.9ms\n",
      "    99th Percentile Latenz: 14.0ms\n",
      "    Worst-Case Latenz: 534.2ms\n",
      "    📋 Overall SLA-Score: 95.8/100\n",
      "\n",
      "  Google:\n",
      "    Excellent Performance: 90.0%\n",
      "    Availability: 100.0%\n",
      "    Reliability: 100.0%\n",
      "    95th Percentile Latenz: 22.0ms\n",
      "    99th Percentile Latenz: 29.8ms\n",
      "    Worst-Case Latenz: 81.4ms\n",
      "    📋 Overall SLA-Score: 96.0/100\n",
      "\n",
      "  Cloudflare:\n",
      "    Excellent Performance: 99.7%\n",
      "    Availability: 100.0%\n",
      "    Reliability: 100.0%\n",
      "    95th Percentile Latenz: 4.8ms\n",
      "    99th Percentile Latenz: 4.9ms\n",
      "    Worst-Case Latenz: 183.6ms\n",
      "    📋 Overall SLA-Score: 99.9/100\n",
      "\n",
      "📉 PERFORMANCE-DEGRADATION-ANALYSE:\n",
      "  Business Hours vs. Off-Hours:\n",
      "    Business Hours Latenz: 2.6ms\n",
      "    Off-Hours Latenz: 2.5ms\n",
      "    Performance-Degradation: 0.4%\n",
      "  Wochenende vs. Werktage:\n",
      "    Wochenende Latenz: 2.5ms\n",
      "    Werktage Latenz: 2.6ms\n",
      "    Wochenend-Effekt: 2.4%\n",
      "🔍 Starte Akamai-Problem Deep-Dive...\n",
      "\n",
      "6. AKAMAI-PROBLEM DEEP-DIVE - IPv4\n",
      "---------------------------------------------\n",
      "\n",
      "🔍 AKAMAI vs. ECHTE ANYCAST ARCHITEKTUR-VERGLEICH:\n",
      "\n",
      "Routing-Diversität-Vergleich:\n",
      "  Akamai Pfade: 22989\n",
      "  Cloudflare Pfade: 45978\n",
      "  Google Pfade: 22989\n",
      "\n",
      "  Akamai:\n",
      "    Gesamte ASNs: 4\n",
      "    Durchschn. ASNs/Region: 2.9\n",
      "    Finale Destinations: 11\n",
      "    Durchschn. Destinations/Region: 1.1\n",
      "    Eindeutige finale IPs: 1\n",
      "\n",
      "  Cloudflare:\n",
      "    Gesamte ASNs: 8\n",
      "    Durchschn. ASNs/Region: 3.0\n",
      "    Finale Destinations: 21\n",
      "    Durchschn. Destinations/Region: 2.1\n",
      "    Eindeutige finale IPs: 1\n",
      "\n",
      "  Google:\n",
      "    Gesamte ASNs: 2\n",
      "    Durchschn. ASNs/Region: 1.8\n",
      "    Finale Destinations: 11\n",
      "    Durchschn. Destinations/Region: 1.1\n",
      "    Eindeutige finale IPs: 1\n",
      "\n",
      "📊 PERFORMANCE-ARCHITEKTUR-KORRELATION:\n",
      "     Provider  Avg_Latency  Latency_Std  P95_Latency  Avg_PacketLoss  \\\n",
      "0      Akamai       145.59        75.41       248.88            0.04   \n",
      "1  Cloudflare         1.82         3.63         4.79            0.00   \n",
      "2      Google         3.69         7.08        21.96            0.00   \n",
      "\n",
      "   Availability  \n",
      "0         100.0  \n",
      "1         100.0  \n",
      "2         100.0  \n",
      "\n",
      "🚨 AKAMAI-PROBLEM-DIAGNOSE:\n",
      "\n",
      "  Routing-Diversität-Defizit:\n",
      "    ASN-Diversität-Defizit: 50.0% weniger als Cloudflare\n",
      "    🔴 PROBLEM: Akamai routet zu nur 1 finaler IP (echtes Unicast)\n",
      "\n",
      "🌍 REGIONALE AKAMAI-INEFFIZIENZ:\n",
      "Schlechteste Akamai-Regionen:\n",
      "  ap-southeast-2: 249.8ms (±4.5ms)\n",
      "  ap-northeast-1: 220.4ms (±4.8ms)\n",
      "  sa-east-1: 188.7ms (±8.3ms)\n",
      "  ap-east-1: 182.6ms (±8.6ms)\n",
      "  ap-south-1: 169.3ms (±6.3ms)\n",
      "\n",
      "📍 AKAMAI vs. GEOGRAFISCHE REFERENZ:\n",
      "  Akamai Durchschn. Latenz: 145.6ms\n",
      "  Unicast Durchschn. Latenz: 153.7ms\n",
      "  Performance-Ratio: 0.95\n",
      "  🔴 BESTÄTIGT: Akamai verhält sich wie Unicast (0.95x)\n",
      "\n",
      "=====================================================================================\n",
      "UMFASSENDE ANALYSE FÜR IPv6\n",
      "=====================================================================================\n",
      "🌐 Starte Netzwerk-Topologie-Analyse...\n",
      "\n",
      "1. NETZWERK-TOPOLOGIE & INFRASTRUKTUR - IPv6\n",
      "------------------------------------------------------------\n",
      "\n",
      "🌐 HOP-BY-HOP NETZWERK-ANALYSE:\n",
      "Netzwerk-Pfade analysiert: 160,923\n",
      "\n",
      "📊 ASN-DIVERSITÄT PRO SERVICE:\n",
      "  Quad9 DNS:\n",
      "    Gesamte ASNs: 6\n",
      "    Durchschn. ASNs/Region: 3.0\n",
      "    Gemeinsame ASNs: 2 (33.3%)\n",
      "  Google DNS:\n",
      "    Gesamte ASNs: 4\n",
      "    Durchschn. ASNs/Region: 2.3\n",
      "    Gemeinsame ASNs: 2 (50.0%)\n",
      "  Cloudflare DNS:\n",
      "    Gesamte ASNs: 5\n",
      "    Durchschn. ASNs/Region: 2.5\n",
      "    Gemeinsame ASNs: 2 (40.0%)\n",
      "  Berkeley NTP:\n",
      "    Gesamte ASNs: 5\n",
      "    Durchschn. ASNs/Region: 4.4\n",
      "    Gemeinsame ASNs: 3 (60.0%)\n",
      "  Heise:\n",
      "    Gesamte ASNs: 7\n",
      "    Durchschn. ASNs/Region: 4.1\n",
      "    Gemeinsame ASNs: 2 (28.6%)\n",
      "  Akamai CDN:\n",
      "    Gesamte ASNs: 6\n",
      "    Durchschn. ASNs/Region: 2.8\n",
      "    Gemeinsame ASNs: 2 (33.3%)\n",
      "  Cloudflare CDN:\n",
      "    Gesamte ASNs: 6\n",
      "    Durchschn. ASNs/Region: 2.6\n",
      "    Gemeinsame ASNs: 2 (33.3%)\n",
      "\n",
      "🏢 TRANSIT-PROVIDER-TIER-ANALYSE:\n",
      "  Hyperscaler: 3 ASNs (AS13335, AS15169, AS16509...)\n",
      "  Tier-1: 5 ASNs (AS174, AS3257, AS3356, AS5511, AS6453...)\n",
      "\n",
      "🔢 HOP-COUNT-ANALYSE:\n",
      "  ANYCAST:\n",
      "    Durchschn. Hops: 9.1\n",
      "    Min-Max Hops: 4-19\n",
      "    Std.Dev.: 2.4\n",
      "  UNICAST:\n",
      "    Durchschn. Hops: 17.6\n",
      "    Min-Max Hops: 6-30\n",
      "    Std.Dev.: 5.1\n",
      "  PSEUDO-ANYCAST:\n",
      "    Durchschn. Hops: 16.8\n",
      "    Min-Max Hops: 8-25\n",
      "    Std.Dev.: 3.7\n",
      "🚨 Starte Anomalie-Deep-Dive...\n",
      "\n",
      "2. ANOMALIE-DEEP-DIVE UND KLASSIFIKATION - IPv6\n",
      "------------------------------------------------------------\n",
      "Performance-Daten für Anomalie-Analyse: 160,923\n",
      "\n",
      "🚨 ERWEITERTE ANOMALIE-KLASSIFIKATION:\n",
      "Gesamte Anomalien: 30,863\n",
      "\n",
      "Anomalie-Typen:\n",
      "  routing_change: 13,695 (44.4%)\n",
      "  latency_spike: 9,094 (29.5%)\n",
      "  jitter_spike: 8,043 (26.1%)\n",
      "  packet_loss: 31 (0.1%)\n",
      "\n",
      "🌍 GEOGRAFISCHE ANOMALIE-HOTSPOTS:\n",
      "  af-south-1: 8,153 Anomalien (50.64% Rate)\n",
      "  eu-north-1: 4,396 Anomalien (27.30% Rate)\n",
      "  ca-central-1: 4,305 Anomalien (26.73% Rate)\n",
      "  ap-east-1: 3,286 Anomalien (20.42% Rate)\n",
      "  ap-south-1: 3,250 Anomalien (20.18% Rate)\n",
      "  us-west-1: 2,743 Anomalien (17.04% Rate)\n",
      "  sa-east-1: 2,154 Anomalien (13.38% Rate)\n",
      "  eu-central-1: 1,153 Anomalien (7.17% Rate)\n",
      "  ap-southeast-2: 791 Anomalien (4.92% Rate)\n",
      "  ap-northeast-1: 632 Anomalien (3.94% Rate)\n",
      "\n",
      "📊 ANOMALIE-VERHALTEN PRO SERVICE-TYP:\n",
      "type            jitter_spike  latency_spike  packet_loss  routing_change\n",
      "service_type                                                            \n",
      "anycast                 4595           9074           17            5176\n",
      "pseudo-anycast          1149              1            1            2681\n",
      "unicast                 2299             19           13            5838\n",
      "\n",
      "⏰ TEMPORALE ANOMALIE-CLUSTER:\n",
      "Peak Anomalie-Stunden: {12: np.int64(1437), 11: np.int64(1387), 13: np.int64(1357)}\n",
      "\n",
      "🏢 PROVIDER-ANOMALIE-PROFILE:\n",
      "  Quad9: 18.99% Anomalie-Rate\n",
      "    Schweregrade: {'high': np.int64(2302), 'medium': np.int64(1182), 'low': np.int64(881)}\n",
      "  Google: 29.52% Anomalie-Rate\n",
      "    Schweregrade: {'high': np.int64(2846), 'low': np.int64(2701), 'medium': np.int64(1239)}\n",
      "  Cloudflare: 16.77% Anomalie-Rate\n",
      "    Schweregrade: {'medium': np.int64(6048), 'low': np.int64(1594), 'high': np.int64(69)}\n",
      "  UC Berkeley: 29.42% Anomalie-Rate\n",
      "    Schweregrade: {'low': np.int64(5609), 'medium': np.int64(1155)}\n",
      "  Heise: 6.11% Anomalie-Rate\n",
      "    Schweregrade: {'medium': np.int64(1172), 'low': np.int64(229), 'high': np.int64(4)}\n",
      "  Akamai: 16.67% Anomalie-Rate\n",
      "    Schweregrade: {'low': np.int64(2681), 'medium': np.int64(1151)}\n",
      "🏗️ Starte Provider-Infrastruktur-Mapping...\n",
      "\n",
      "3. PROVIDER-INFRASTRUKTUR-MAPPING - IPv6\n",
      "-------------------------------------------------------\n",
      "\n",
      "🏗️ PROVIDER-EDGE-INFRASTRUKTUR-ANALYSE:\n",
      "\n",
      "📍 EDGE-SERVER-DENSITY:\n",
      "\n",
      "  Cloudflare:\n",
      "    Gesamte Edge-Server: 20\n",
      "    Regionen abgedeckt: 10\n",
      "    Durchschn. Edges/Region: 2.0\n",
      "    Top-Regionen: {'sa-east-1': 2, 'ap-east-1': 2, 'eu-central-1': 2}\n",
      "    Geografische Indikatoren: 3\n",
      "\n",
      "  Google:\n",
      "    Gesamte Edge-Server: 10\n",
      "    Regionen abgedeckt: 10\n",
      "    Durchschn. Edges/Region: 1.0\n",
      "    Top-Regionen: {'af-south-1': 1, 'ap-east-1': 1, 'eu-north-1': 1}\n",
      "    Geografische Indikatoren: 2\n",
      "\n",
      "  Quad9:\n",
      "    Gesamte Edge-Server: 10\n",
      "    Regionen abgedeckt: 10\n",
      "    Durchschn. Edges/Region: 1.0\n",
      "    Top-Regionen: {'ap-east-1': 1, 'eu-north-1': 1, 'sa-east-1': 1}\n",
      "    Geografische Indikatoren: 7\n",
      "\n",
      "📊 PROVIDER-INFRASTRUKTUR-VERGLEICH:\n",
      "     Provider  Total_Edges  Regions  Avg_Edges_Per_Region  Geo_Coverage  \\\n",
      "0  Cloudflare           20       10                   2.0             3   \n",
      "1      Google           10       10                   1.0             2   \n",
      "2       Quad9           10       10                   1.0             7   \n",
      "\n",
      "   Anomaly_Rate_%  \n",
      "0           16.77  \n",
      "1           29.52  \n",
      "2           18.99  \n",
      "\n",
      "⭐ INFRASTRUKTUR-EFFIZIENZ-RANKING:\n",
      "  Cloudflare: 1.7/10\n",
      "    Edge-Density: 2.0/10\n",
      "    Geo-Coverage: 1.5/10\n",
      "    Stabilität: 1.6/10\n",
      "  Google: 0.7/10\n",
      "    Edge-Density: 1.0/10\n",
      "    Geo-Coverage: 1.0/10\n",
      "    Stabilität: 0.0/10\n",
      "  Quad9: 1.7/10\n",
      "    Edge-Density: 1.0/10\n",
      "    Geo-Coverage: 3.5/10\n",
      "    Stabilität: 0.5/10\n",
      "📊 Starte Statistische & Prädiktive Analysen...\n",
      "\n",
      "4. STATISTISCHE & PRÄDIKTIVE ANALYSEN - IPv6\n",
      "-------------------------------------------------------\n",
      "\n",
      "📈 KORRELATIONSANALYSE:\n",
      "Korrelations-Matrix:\n",
      "                       latency  packet_loss  jitter  intermediate_failures  \\\n",
      "latency                  1.000        0.034   0.089                  0.366   \n",
      "packet_loss              0.034        1.000   0.034                  0.010   \n",
      "jitter                   0.089        0.034   1.000                  0.044   \n",
      "intermediate_failures    0.366        0.010   0.044                  1.000   \n",
      "total_hops               0.732        0.015   0.051                  0.716   \n",
      "\n",
      "                       total_hops  \n",
      "latency                     0.732  \n",
      "packet_loss                 0.015  \n",
      "jitter                      0.051  \n",
      "intermediate_failures       0.716  \n",
      "total_hops                  1.000  \n",
      "\n",
      "🔍 SIGNIFIKANTE KORRELATIONEN (|r| > 0.3):\n",
      "  latency ↔ intermediate_failures: 0.366\n",
      "  latency ↔ total_hops: 0.732\n",
      "  intermediate_failures ↔ total_hops: 0.716\n",
      "\n",
      "🕰️ TIME-SERIES-CLUSTERING:\n",
      "Provider-Performance-Cluster:\n",
      "  Cluster 0: ['Google']\n",
      "    Durchschn. Latenz: 5.60ms\n",
      "    Durchschn. Stabilität: 12.01ms\n",
      "  Cluster 1: ['Cloudflare']\n",
      "    Durchschn. Latenz: 2.04ms\n",
      "    Durchschn. Stabilität: 4.46ms\n",
      "  Cluster 2: ['Quad9']\n",
      "    Durchschn. Latenz: 3.09ms\n",
      "    Durchschn. Stabilität: 3.87ms\n",
      "\n",
      "🔮 PREDICTIVE MODELING:\n",
      "  Latenz-Vorhersage-Modell:\n",
      "    R² Score: 0.827\n",
      "    RMSE: 36.728ms\n",
      "    Feature-Wichtigkeit:\n",
      "      hour: 0.010\n",
      "      day_of_week_num: 0.007\n",
      "      total_hops: 0.198\n",
      "      service_type_numeric: 0.785\n",
      "\n",
      "  Performance-Kategorie-Vorhersage:\n",
      "    Latenz-Kategorien:\n",
      "      Excellent: 93,419 (58.1%)\n",
      "      Poor: 51,441 (32.0%)\n",
      "      Fair: 12,851 (8.0%)\n",
      "      Good: 3,212 (2.0%)\n",
      "📋 Starte Qualitäts- und SLA-Analysen...\n",
      "\n",
      "5. QUALITÄTS- UND SLA-ANALYSEN - IPv6\n",
      "---------------------------------------------\n",
      "\n",
      "📊 SLA-COMPLIANCE-ANALYSE:\n",
      "SLA-Schwellwerte:\n",
      "  latency_excellent: 5\n",
      "  latency_good: 20\n",
      "  latency_acceptable: 100\n",
      "  packet_loss_max: 1\n",
      "  availability_min: 99.9\n",
      "\n",
      "SLA-Compliance-Übersicht:\n",
      "     Service_Type  Excellent_Latency_%  Good_Latency_%  Acceptable_Latency_%  \\\n",
      "0         anycast                 94.1            97.5                 100.0   \n",
      "1         unicast                  9.9            10.0                  24.9   \n",
      "2  pseudo-anycast                 10.0            10.3                  26.5   \n",
      "\n",
      "   Low_PacketLoss_%  Availability_%  \n",
      "0              99.9           100.0  \n",
      "1              99.7           100.0  \n",
      "2             100.0           100.0  \n",
      "\n",
      "🏢 PROVIDER-SLA-SCORECARD:\n",
      "\n",
      "  Quad9:\n",
      "    Excellent Performance: 89.8%\n",
      "    Availability: 100.0%\n",
      "    Reliability: 100.0%\n",
      "    95th Percentile Latenz: 13.9ms\n",
      "    99th Percentile Latenz: 14.1ms\n",
      "    Worst-Case Latenz: 160.2ms\n",
      "    📋 Overall SLA-Score: 95.9/100\n",
      "\n",
      "  Google:\n",
      "    Excellent Performance: 87.2%\n",
      "    Availability: 100.0%\n",
      "    Reliability: 99.9%\n",
      "    95th Percentile Latenz: 28.3ms\n",
      "    99th Percentile Latenz: 69.0ms\n",
      "    Worst-Case Latenz: 71.8ms\n",
      "    📋 Overall SLA-Score: 94.8/100\n",
      "\n",
      "  Cloudflare:\n",
      "    Excellent Performance: 99.7%\n",
      "    Availability: 100.0%\n",
      "    Reliability: 99.9%\n",
      "    95th Percentile Latenz: 4.8ms\n",
      "    99th Percentile Latenz: 4.9ms\n",
      "    Worst-Case Latenz: 184.5ms\n",
      "    📋 Overall SLA-Score: 99.9/100\n",
      "\n",
      "📉 PERFORMANCE-DEGRADATION-ANALYSE:\n",
      "  Business Hours vs. Off-Hours:\n",
      "    Business Hours Latenz: 3.2ms\n",
      "    Off-Hours Latenz: 3.2ms\n",
      "    Performance-Degradation: 0.7%\n",
      "  Wochenende vs. Werktage:\n",
      "    Wochenende Latenz: 3.3ms\n",
      "    Werktage Latenz: 3.2ms\n",
      "    Wochenend-Effekt: -4.2%\n",
      "🔍 Starte Akamai-Problem Deep-Dive...\n",
      "\n",
      "6. AKAMAI-PROBLEM DEEP-DIVE - IPv6\n",
      "---------------------------------------------\n",
      "\n",
      "🔍 AKAMAI vs. ECHTE ANYCAST ARCHITEKTUR-VERGLEICH:\n",
      "\n",
      "Routing-Diversität-Vergleich:\n",
      "  Akamai Pfade: 22989\n",
      "  Cloudflare Pfade: 45978\n",
      "  Google Pfade: 22989\n",
      "\n",
      "  Akamai:\n",
      "    Gesamte ASNs: 6\n",
      "    Durchschn. ASNs/Region: 2.8\n",
      "    Finale Destinations: 11\n",
      "    Durchschn. Destinations/Region: 1.1\n",
      "    Eindeutige finale IPs: 1\n",
      "\n",
      "  Cloudflare:\n",
      "    Gesamte ASNs: 6\n",
      "    Durchschn. ASNs/Region: 2.6\n",
      "    Finale Destinations: 21\n",
      "    Durchschn. Destinations/Region: 2.1\n",
      "    Eindeutige finale IPs: 1\n",
      "\n",
      "  Google:\n",
      "    Gesamte ASNs: 4\n",
      "    Durchschn. ASNs/Region: 2.3\n",
      "    Finale Destinations: 11\n",
      "    Durchschn. Destinations/Region: 1.1\n",
      "    Eindeutige finale IPs: 1\n",
      "\n",
      "📊 PERFORMANCE-ARCHITEKTUR-KORRELATION:\n",
      "     Provider  Avg_Latency  Latency_Std  P95_Latency  Avg_PacketLoss  \\\n",
      "0      Akamai       144.65        77.10       246.72            0.01   \n",
      "1  Cloudflare         2.04         4.46         4.75            0.01   \n",
      "2      Google         5.60        12.01        28.28            0.06   \n",
      "\n",
      "   Availability  \n",
      "0         100.0  \n",
      "1         100.0  \n",
      "2         100.0  \n",
      "\n",
      "🚨 AKAMAI-PROBLEM-DIAGNOSE:\n",
      "\n",
      "  Routing-Diversität-Defizit:\n",
      "    ASN-Diversität-Defizit: 0.0% weniger als Cloudflare\n",
      "    🔴 PROBLEM: Akamai routet zu nur 1 finaler IP (echtes Unicast)\n",
      "\n",
      "🌍 REGIONALE AKAMAI-INEFFIZIENZ:\n",
      "Schlechteste Akamai-Regionen:\n",
      "  ap-southeast-2: 247.8ms (±3.6ms)\n",
      "  ap-northeast-1: 225.5ms (±4.0ms)\n",
      "  ap-east-1: 190.4ms (±10.1ms)\n",
      "  sa-east-1: 186.4ms (±1.2ms)\n",
      "  ap-south-1: 171.1ms (±6.1ms)\n",
      "\n",
      "📍 AKAMAI vs. GEOGRAFISCHE REFERENZ:\n",
      "  Akamai Durchschn. Latenz: 144.6ms\n",
      "  Unicast Durchschn. Latenz: 149.1ms\n",
      "  Performance-Ratio: 0.97\n",
      "  🔴 BESTÄTIGT: Akamai verhält sich wie Unicast (0.97x)\n",
      "\n",
      "=====================================================================================\n",
      "🎯 ALLE ERWEITERTEN ANALYSEN ABGESCHLOSSEN!\n",
      "🏆 VOLLSTÄNDIGE ANYCAST-FORSCHUNGSSTUDIE ERSTELLT!\n",
      "=====================================================================================\n",
      "\n",
      "📋 KOMPLETTE ANALYSE-ÜBERSICHT:\n",
      "✅ Phase 1: Datenverständnis & Überblick\n",
      "✅ Phase 2: Geografische Routing-Analyse\n",
      "✅ Phase 3: Performance-Trends & Zeitanalyse\n",
      "✅ Phase 4a: Netzwerk-Topologie & Infrastruktur\n",
      "✅ Phase 4b: Anomalie-Deep-Dive & Klassifikation\n",
      "✅ Phase 4c: Provider-Infrastruktur-Mapping\n",
      "✅ Phase 4d: Statistische & Prädiktive Analysen\n",
      "✅ Phase 4e: Qualitäts- & SLA-Analysen\n",
      "✅ Phase 4f: Akamai-Problem Deep-Dive\n",
      "\n",
      "🚀 BEREIT FÜR:\n",
      "  • Wissenschaftliche Publikation\n",
      "  • Konferenz-Präsentation\n",
      "  • Industry-Report\n",
      "  • PhD-Dissertation-Kapitel\n"
     ]
    }
   ],
   "source": [
    "# Phase 4: Umfassende Erweiterte Analysen - Komplette MTR Anycast Studie\n",
    "# ===============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Erweiterte Bibliotheken für komplexe Analysen\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict, Counter\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "print(\"=== PHASE 4: UMFASSENDE ERWEITERTE ANALYSEN ===\")\n",
    "print(\"Netzwerk-Topologie, Anomalie-Deep-Dive, Predictive Analytics & Qualitätsanalysen\")\n",
    "print(\"=\"*85)\n",
    "\n",
    "# ================================================================\n",
    "# 1. NETZWERK-TOPOLOGIE & INFRASTRUKTUR-ANALYSE\n",
    "# ================================================================\n",
    "\n",
    "def analyze_network_topology(df, protocol_name):\n",
    "    \"\"\"Detaillierte Netzwerk-Topologie und Infrastruktur-Analyse\"\"\"\n",
    "    print(f\"\\n1. NETZWERK-TOPOLOGIE & INFRASTRUKTUR - {protocol_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Service-Klassifikation\n",
    "    SERVICE_MAPPING = {\n",
    "        # IPv4\n",
    "        '1.1.1.1': {'name': 'Cloudflare DNS', 'type': 'anycast', 'provider': 'Cloudflare'},\n",
    "        '8.8.8.8': {'name': 'Google DNS', 'type': 'anycast', 'provider': 'Google'}, \n",
    "        '9.9.9.9': {'name': 'Quad9 DNS', 'type': 'anycast', 'provider': 'Quad9'},\n",
    "        '104.16.123.96': {'name': 'Cloudflare CDN', 'type': 'anycast', 'provider': 'Cloudflare'},\n",
    "        '2.16.241.219': {'name': 'Akamai CDN', 'type': 'pseudo-anycast', 'provider': 'Akamai'},\n",
    "        '193.99.144.85': {'name': 'Heise', 'type': 'unicast', 'provider': 'Heise'},\n",
    "        '169.229.128.134': {'name': 'Berkeley NTP', 'type': 'unicast', 'provider': 'UC Berkeley'},\n",
    "        \n",
    "        # IPv6\n",
    "        '2606:4700:4700::1111': {'name': 'Cloudflare DNS', 'type': 'anycast', 'provider': 'Cloudflare'},\n",
    "        '2001:4860:4860::8888': {'name': 'Google DNS', 'type': 'anycast', 'provider': 'Google'},\n",
    "        '2620:fe::fe:9': {'name': 'Quad9 DNS', 'type': 'anycast', 'provider': 'Quad9'}, \n",
    "        '2606:4700::6810:7b60': {'name': 'Cloudflare CDN', 'type': 'anycast', 'provider': 'Cloudflare'},\n",
    "        '2a02:26f0:3500:1b::1724:a393': {'name': 'Akamai CDN', 'type': 'pseudo-anycast', 'provider': 'Akamai'},\n",
    "        '2a02:2e0:3fe:1001:7777:772e:2:85': {'name': 'Heise', 'type': 'unicast', 'provider': 'Heise'},\n",
    "        '2607:f140:ffff:8000:0:8006:0:a': {'name': 'Berkeley NTP', 'type': 'unicast', 'provider': 'UC Berkeley'}\n",
    "    }\n",
    "    \n",
    "    df['service_info'] = df['dst'].map(SERVICE_MAPPING)\n",
    "    df['service_name'] = df['service_info'].apply(lambda x: x['name'] if x else 'Unknown')\n",
    "    df['service_type'] = df['service_info'].apply(lambda x: x['type'] if x else 'Unknown')\n",
    "    df['provider'] = df['service_info'].apply(lambda x: x['provider'] if x else 'Unknown')\n",
    "    \n",
    "    # Netzwerk-Pfad-Extraktion\n",
    "    network_paths = []\n",
    "    asn_analysis = defaultdict(lambda: defaultdict(set))\n",
    "    tier_classification = defaultdict(set)\n",
    "    \n",
    "    print(f\"\\n🌐 HOP-BY-HOP NETZWERK-ANALYSE:\")\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            if row['hubs'] is not None and len(row['hubs']) > 0:\n",
    "                path_info = {\n",
    "                    'service': row['service_name'],\n",
    "                    'service_type': row['service_type'],\n",
    "                    'provider': row['provider'],\n",
    "                    'region': row['region'],\n",
    "                    'hops': [],\n",
    "                    'asns': [],\n",
    "                    'latencies': [],\n",
    "                    'geographic_indicators': []\n",
    "                }\n",
    "                \n",
    "                for i, hop in enumerate(row['hubs']):\n",
    "                    if hop:\n",
    "                        hop_info = {\n",
    "                            'hop_number': i + 1,\n",
    "                            'hostname': hop.get('host', '???'),\n",
    "                            'asn': hop.get('ASN', 'AS???'),\n",
    "                            'latency': hop.get('Avg', 0),\n",
    "                            'loss': hop.get('Loss%', 0)\n",
    "                        }\n",
    "                        \n",
    "                        path_info['hops'].append(hop_info)\n",
    "                        \n",
    "                        # ASN-Analyse\n",
    "                        if hop.get('ASN') and hop.get('ASN') != 'AS???':\n",
    "                            asn = hop.get('ASN')\n",
    "                            path_info['asns'].append(asn)\n",
    "                            asn_analysis[row['service_name']][row['region']].add(asn)\n",
    "                            \n",
    "                            # Tier-Klassifikation basierend auf bekannten ASNs\n",
    "                            if asn in ['AS174', 'AS3356', 'AS1299', 'AS3257', 'AS6453', 'AS5511']:\n",
    "                                tier_classification['Tier-1'].add(asn)\n",
    "                            elif asn in ['AS16509', 'AS13335', 'AS15169']:  # AWS, Cloudflare, Google\n",
    "                                tier_classification['Hyperscaler'].add(asn)\n",
    "                        \n",
    "                        # Geografische Indikatoren aus Hostnames\n",
    "                        hostname = hop.get('host', '')\n",
    "                        if hostname and hostname != '???':\n",
    "                            geo_indicators = extract_geographic_indicators(hostname)\n",
    "                            path_info['geographic_indicators'].extend(geo_indicators)\n",
    "                        \n",
    "                        if hop.get('Avg', 0) > 0:\n",
    "                            path_info['latencies'].append(hop.get('Avg', 0))\n",
    "                \n",
    "                network_paths.append(path_info)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Netzwerk-Pfade analysiert: {len(network_paths):,}\")\n",
    "    \n",
    "    # ASN-Diversität-Analyse\n",
    "    print(f\"\\n📊 ASN-DIVERSITÄT PRO SERVICE:\")\n",
    "    \n",
    "    for service in asn_analysis.keys():\n",
    "        total_asns = set()\n",
    "        for region_asns in asn_analysis[service].values():\n",
    "            total_asns.update(region_asns)\n",
    "        \n",
    "        avg_asns_per_region = np.mean([len(asns) for asns in asn_analysis[service].values()])\n",
    "        print(f\"  {service}:\")\n",
    "        print(f\"    Gesamte ASNs: {len(total_asns)}\")\n",
    "        print(f\"    Durchschn. ASNs/Region: {avg_asns_per_region:.1f}\")\n",
    "        \n",
    "        # ASN-Überlappung zwischen Regionen\n",
    "        all_region_asns = list(asn_analysis[service].values())\n",
    "        if len(all_region_asns) > 1:\n",
    "            intersection = set.intersection(*all_region_asns)\n",
    "            print(f\"    Gemeinsame ASNs: {len(intersection)} ({len(intersection)/len(total_asns)*100:.1f}%)\")\n",
    "    \n",
    "    # Tier-Analyse\n",
    "    print(f\"\\n🏢 TRANSIT-PROVIDER-TIER-ANALYSE:\")\n",
    "    for tier, asns in tier_classification.items():\n",
    "        print(f\"  {tier}: {len(asns)} ASNs ({', '.join(sorted(asns)[:5])}...)\")\n",
    "    \n",
    "    # Hop-Count-Analyse\n",
    "    print(f\"\\n🔢 HOP-COUNT-ANALYSE:\")\n",
    "    \n",
    "    hop_analysis = defaultdict(list)\n",
    "    for path in network_paths:\n",
    "        hop_analysis[path['service_type']].append(len(path['hops']))\n",
    "    \n",
    "    for service_type, hop_counts in hop_analysis.items():\n",
    "        if hop_counts:\n",
    "            print(f\"  {service_type.upper()}:\")\n",
    "            print(f\"    Durchschn. Hops: {np.mean(hop_counts):.1f}\")\n",
    "            print(f\"    Min-Max Hops: {min(hop_counts)}-{max(hop_counts)}\")\n",
    "            print(f\"    Std.Dev.: {np.std(hop_counts):.1f}\")\n",
    "    \n",
    "    return network_paths, asn_analysis\n",
    "\n",
    "def extract_geographic_indicators(hostname):\n",
    "    \"\"\"Extrahiert geografische Indikatoren aus Hostnames\"\"\"\n",
    "    geo_patterns = {\n",
    "        'cities': ['nyc', 'lax', 'dfw', 'ord', 'iad', 'lhr', 'fra', 'nrt', 'sin', 'syd'],\n",
    "        'countries': ['us', 'de', 'uk', 'jp', 'au', 'ca', 'fr', 'nl', 'se', 'br'],\n",
    "        'regions': ['east', 'west', 'north', 'south', 'central', 'europe', 'asia', 'america']\n",
    "    }\n",
    "    \n",
    "    indicators = []\n",
    "    hostname_lower = hostname.lower()\n",
    "    \n",
    "    for category, patterns in geo_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in hostname_lower:\n",
    "                indicators.append(f\"{category}:{pattern}\")\n",
    "    \n",
    "    return indicators\n",
    "\n",
    "# ================================================================\n",
    "# 2. ANOMALIE-DEEP-DIVE UND KLASSIFIKATION\n",
    "# ================================================================\n",
    "\n",
    "def comprehensive_anomaly_analysis(df, protocol_name):\n",
    "    \"\"\"Umfassende Anomalie-Analyse und Klassifikation\"\"\"\n",
    "    print(f\"\\n2. ANOMALIE-DEEP-DIVE UND KLASSIFIKATION - {protocol_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Service-Klassifikation (gleich wie oben)\n",
    "    SERVICE_MAPPING = {\n",
    "        # [Same mapping as above]\n",
    "        '1.1.1.1': {'name': 'Cloudflare DNS', 'type': 'anycast', 'provider': 'Cloudflare'},\n",
    "        '8.8.8.8': {'name': 'Google DNS', 'type': 'anycast', 'provider': 'Google'}, \n",
    "        '9.9.9.9': {'name': 'Quad9 DNS', 'type': 'anycast', 'provider': 'Quad9'},\n",
    "        '104.16.123.96': {'name': 'Cloudflare CDN', 'type': 'anycast', 'provider': 'Cloudflare'},\n",
    "        '2.16.241.219': {'name': 'Akamai CDN', 'type': 'pseudo-anycast', 'provider': 'Akamai'},\n",
    "        '193.99.144.85': {'name': 'Heise', 'type': 'unicast', 'provider': 'Heise'},\n",
    "        '169.229.128.134': {'name': 'Berkeley NTP', 'type': 'unicast', 'provider': 'UC Berkeley'},\n",
    "        '2606:4700:4700::1111': {'name': 'Cloudflare DNS', 'type': 'anycast', 'provider': 'Cloudflare'},\n",
    "        '2001:4860:4860::8888': {'name': 'Google DNS', 'type': 'anycast', 'provider': 'Google'},\n",
    "        '2620:fe::fe:9': {'name': 'Quad9 DNS', 'type': 'anycast', 'provider': 'Quad9'}, \n",
    "        '2606:4700::6810:7b60': {'name': 'Cloudflare CDN', 'type': 'anycast', 'provider': 'Cloudflare'},\n",
    "        '2a02:26f0:3500:1b::1724:a393': {'name': 'Akamai CDN', 'type': 'pseudo-anycast', 'provider': 'Akamai'},\n",
    "        '2a02:2e0:3fe:1001:7777:772e:2:85': {'name': 'Heise', 'type': 'unicast', 'provider': 'Heise'},\n",
    "        '2607:f140:ffff:8000:0:8006:0:a': {'name': 'Berkeley NTP', 'type': 'unicast', 'provider': 'UC Berkeley'}\n",
    "    }\n",
    "    \n",
    "    df['service_info'] = df['dst'].map(SERVICE_MAPPING)\n",
    "    df['service_name'] = df['service_info'].apply(lambda x: x['name'] if x else 'Unknown')\n",
    "    df['service_type'] = df['service_info'].apply(lambda x: x['type'] if x else 'Unknown')\n",
    "    df['provider'] = df['service_info'].apply(lambda x: x['provider'] if x else 'Unknown')\n",
    "    df['utctime'] = pd.to_datetime(df['utctime'])\n",
    "    \n",
    "    # Performance-Metriken extrahieren\n",
    "    performance_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            if row['hubs'] is not None and len(row['hubs']) > 0:\n",
    "                final_latency = None\n",
    "                final_loss = None\n",
    "                final_jitter = None\n",
    "                routing_changes = 0\n",
    "                intermediate_failures = 0\n",
    "                \n",
    "                # Erweiterte Metriken\n",
    "                for i, hop in enumerate(row['hubs']):\n",
    "                    if hop and hop.get('Loss%', 0) == 100:\n",
    "                        intermediate_failures += 1\n",
    "                    \n",
    "                    if hop and hop.get('Avg') and hop.get('Avg') > 0:\n",
    "                        final_latency = hop.get('Avg', 0)\n",
    "                        final_loss = hop.get('Loss%', 0)\n",
    "                        final_jitter = hop.get('Javg', 0)\n",
    "                \n",
    "                if final_latency is not None:\n",
    "                    performance_data.append({\n",
    "                        'timestamp': row['utctime'],\n",
    "                        'service_name': row['service_name'],\n",
    "                        'service_type': row['service_type'],\n",
    "                        'provider': row['provider'],\n",
    "                        'region': row['region'],\n",
    "                        'latency': final_latency,\n",
    "                        'packet_loss': final_loss,\n",
    "                        'jitter': final_jitter if final_jitter else 0,\n",
    "                        'intermediate_failures': intermediate_failures,\n",
    "                        'total_hops': len(row['hubs'])\n",
    "                    })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    perf_df = pd.DataFrame(performance_data)\n",
    "    \n",
    "    if len(perf_df) == 0:\n",
    "        print(\"Keine Performance-Daten für Anomalie-Analyse\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Performance-Daten für Anomalie-Analyse: {len(perf_df):,}\")\n",
    "    \n",
    "    # Erweiterte Anomalie-Klassifikation\n",
    "    anomalies = []\n",
    "    \n",
    "    print(f\"\\n🚨 ERWEITERTE ANOMALIE-KLASSIFIKATION:\")\n",
    "    \n",
    "    for service in perf_df['service_name'].unique():\n",
    "        service_data = perf_df[perf_df['service_name'] == service].copy()\n",
    "        \n",
    "        if len(service_data) < 100:\n",
    "            continue\n",
    "        \n",
    "        # Multiple Anomalie-Typen\n",
    "        \n",
    "        # 1. Latenz-Anomalien (IQR-Methode)\n",
    "        Q1 = service_data['latency'].quantile(0.25)\n",
    "        Q3 = service_data['latency'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        \n",
    "        latency_anomalies = service_data[service_data['latency'] > upper_bound]\n",
    "        \n",
    "        # 2. Jitter-Anomalien\n",
    "        jitter_threshold = service_data['jitter'].quantile(0.95)\n",
    "        jitter_anomalies = service_data[service_data['jitter'] > jitter_threshold]\n",
    "        \n",
    "        # 3. Packet-Loss-Anomalien\n",
    "        loss_anomalies = service_data[service_data['packet_loss'] > 50]\n",
    "        \n",
    "        # 4. Routing-Anomalien (ungewöhnliche Hop-Counts)\n",
    "        hop_median = service_data['total_hops'].median()\n",
    "        routing_anomalies = service_data[abs(service_data['total_hops'] - hop_median) > 5]\n",
    "        \n",
    "        # Anomalien sammeln\n",
    "        for _, row in latency_anomalies.iterrows():\n",
    "            anomalies.append({\n",
    "                'service': service,\n",
    "                'service_type': row['service_type'],\n",
    "                'provider': row['provider'],\n",
    "                'region': row['region'],\n",
    "                'timestamp': row['timestamp'],\n",
    "                'type': 'latency_spike',\n",
    "                'value': row['latency'],\n",
    "                'threshold': upper_bound,\n",
    "                'severity': 'high' if row['latency'] > upper_bound * 2 else 'medium'\n",
    "            })\n",
    "        \n",
    "        for _, row in jitter_anomalies.iterrows():\n",
    "            anomalies.append({\n",
    "                'service': service,\n",
    "                'service_type': row['service_type'],\n",
    "                'provider': row['provider'],\n",
    "                'region': row['region'],\n",
    "                'timestamp': row['timestamp'],\n",
    "                'type': 'jitter_spike',\n",
    "                'value': row['jitter'],\n",
    "                'threshold': jitter_threshold,\n",
    "                'severity': 'medium'\n",
    "            })\n",
    "        \n",
    "        for _, row in loss_anomalies.iterrows():\n",
    "            anomalies.append({\n",
    "                'service': service,\n",
    "                'service_type': row['service_type'],\n",
    "                'provider': row['provider'],\n",
    "                'region': row['region'],\n",
    "                'timestamp': row['timestamp'],\n",
    "                'type': 'packet_loss',\n",
    "                'value': row['packet_loss'],\n",
    "                'threshold': 50,\n",
    "                'severity': 'high' if row['packet_loss'] > 80 else 'medium'\n",
    "            })\n",
    "        \n",
    "        for _, row in routing_anomalies.iterrows():\n",
    "            anomalies.append({\n",
    "                'service': service,\n",
    "                'service_type': row['service_type'],\n",
    "                'provider': row['provider'],\n",
    "                'region': row['region'],\n",
    "                'timestamp': row['timestamp'],\n",
    "                'type': 'routing_change',\n",
    "                'value': row['total_hops'],\n",
    "                'threshold': hop_median,\n",
    "                'severity': 'low'\n",
    "            })\n",
    "    \n",
    "    if anomalies:\n",
    "        anomalies_df = pd.DataFrame(anomalies)\n",
    "        \n",
    "        print(f\"Gesamte Anomalien: {len(anomalies):,}\")\n",
    "        \n",
    "        # Anomalie-Typ-Verteilung\n",
    "        print(f\"\\nAnomalie-Typen:\")\n",
    "        type_counts = anomalies_df['type'].value_counts()\n",
    "        for anomaly_type, count in type_counts.items():\n",
    "            print(f\"  {anomaly_type}: {count:,} ({count/len(anomalies)*100:.1f}%)\")\n",
    "        \n",
    "        # Geografische Anomalie-Hotspots\n",
    "        print(f\"\\n🌍 GEOGRAFISCHE ANOMALIE-HOTSPOTS:\")\n",
    "        region_anomalies = anomalies_df.groupby('region').size().sort_values(ascending=False)\n",
    "        for region, count in region_anomalies.head(10).items():\n",
    "            total_measurements = len(perf_df[perf_df['region'] == region])\n",
    "            rate = (count / total_measurements) * 100 if total_measurements > 0 else 0\n",
    "            print(f\"  {region}: {count:,} Anomalien ({rate:.2f}% Rate)\")\n",
    "        \n",
    "        # Service-Typ Anomalie-Verhalten\n",
    "        print(f\"\\n📊 ANOMALIE-VERHALTEN PRO SERVICE-TYP:\")\n",
    "        \n",
    "        type_analysis = anomalies_df.groupby(['service_type', 'type']).size().unstack(fill_value=0)\n",
    "        print(type_analysis)\n",
    "        \n",
    "        # Temporale Anomalie-Cluster\n",
    "        print(f\"\\n⏰ TEMPORALE ANOMALIE-CLUSTER:\")\n",
    "        anomalies_df['hour'] = anomalies_df['timestamp'].dt.hour\n",
    "        anomalies_df['day_of_week'] = anomalies_df['timestamp'].dt.day_name()\n",
    "        \n",
    "        hourly_anomalies = anomalies_df.groupby('hour').size()\n",
    "        peak_hours = hourly_anomalies.nlargest(3)\n",
    "        print(f\"Peak Anomalie-Stunden: {dict(peak_hours)}\")\n",
    "        \n",
    "        # Provider-spezifische Anomalie-Raten\n",
    "        print(f\"\\n🏢 PROVIDER-ANOMALIE-PROFILE:\")\n",
    "        \n",
    "        for provider in perf_df['provider'].unique():\n",
    "            provider_data = perf_df[perf_df['provider'] == provider]\n",
    "            provider_anomalies = anomalies_df[anomalies_df['provider'] == provider]\n",
    "            \n",
    "            if len(provider_data) > 0:\n",
    "                anomaly_rate = len(provider_anomalies) / len(provider_data) * 100\n",
    "                print(f\"  {provider}: {anomaly_rate:.2f}% Anomalie-Rate\")\n",
    "                \n",
    "                if len(provider_anomalies) > 0:\n",
    "                    severity_dist = provider_anomalies['severity'].value_counts()\n",
    "                    print(f\"    Schweregrade: {dict(severity_dist)}\")\n",
    "        \n",
    "        return anomalies_df, perf_df\n",
    "    \n",
    "    else:\n",
    "        print(\"Keine signifikanten Anomalien entdeckt\")\n",
    "        return None, perf_df\n",
    "\n",
    "# ================================================================\n",
    "# 3. PROVIDER-INFRASTRUKTUR-MAPPING\n",
    "# ================================================================\n",
    "\n",
    "def map_provider_infrastructure(network_paths, anomalies_df, protocol_name):\n",
    "    \"\"\"Detailliertes Provider-Infrastruktur-Mapping\"\"\"\n",
    "    print(f\"\\n3. PROVIDER-INFRASTRUKTUR-MAPPING - {protocol_name}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    if not network_paths:\n",
    "        print(\"Keine Netzwerk-Pfad-Daten verfügbar\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🏗️ PROVIDER-EDGE-INFRASTRUKTUR-ANALYSE:\")\n",
    "    \n",
    "    # Provider-Edge-Density-Mapping\n",
    "    provider_edges = defaultdict(lambda: defaultdict(set))\n",
    "    geographic_coverage = defaultdict(lambda: defaultdict(set))\n",
    "    \n",
    "    for path in network_paths:\n",
    "        if path['service_type'] == 'anycast':\n",
    "            provider = path['provider']\n",
    "            region = path['region']\n",
    "            \n",
    "            # Finale Hops als Edge-Server identifizieren\n",
    "            if path['hops']:\n",
    "                final_hop = path['hops'][-1]\n",
    "                if final_hop['hostname'] != '???':\n",
    "                    provider_edges[provider][region].add(final_hop['hostname'])\n",
    "            \n",
    "            # Geografische Indikatoren sammeln\n",
    "            for geo_indicator in path['geographic_indicators']:\n",
    "                geographic_coverage[provider][region].add(geo_indicator)\n",
    "    \n",
    "    # Edge-Density-Analyse\n",
    "    print(f\"\\n📍 EDGE-SERVER-DENSITY:\")\n",
    "    \n",
    "    for provider in sorted(provider_edges.keys()):\n",
    "        print(f\"\\n  {provider}:\")\n",
    "        total_edges = sum(len(edges) for edges in provider_edges[provider].values())\n",
    "        regions_covered = len(provider_edges[provider])\n",
    "        \n",
    "        print(f\"    Gesamte Edge-Server: {total_edges}\")\n",
    "        print(f\"    Regionen abgedeckt: {regions_covered}\")\n",
    "        print(f\"    Durchschn. Edges/Region: {total_edges/regions_covered:.1f}\")\n",
    "        \n",
    "        # Top-Regionen für diesen Provider\n",
    "        region_edge_counts = {region: len(edges) for region, edges in provider_edges[provider].items()}\n",
    "        top_regions = sorted(region_edge_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        \n",
    "        print(f\"    Top-Regionen: {dict(top_regions)}\")\n",
    "        \n",
    "        # Geografische Abdeckung\n",
    "        all_geo = set()\n",
    "        for geo_set in geographic_coverage[provider].values():\n",
    "            all_geo.update(geo_set)\n",
    "        print(f\"    Geografische Indikatoren: {len(all_geo)}\")\n",
    "    \n",
    "    # Provider-Vergleichsmatrix\n",
    "    print(f\"\\n📊 PROVIDER-INFRASTRUKTUR-VERGLEICH:\")\n",
    "    \n",
    "    comparison_matrix = []\n",
    "    \n",
    "    for provider in sorted(provider_edges.keys()):\n",
    "        total_edges = sum(len(edges) for edges in provider_edges[provider].values())\n",
    "        regions = len(provider_edges[provider])\n",
    "        avg_edges = total_edges / regions if regions > 0 else 0\n",
    "        geo_coverage = len(set().union(*geographic_coverage[provider].values()))\n",
    "        \n",
    "        # Anomalie-Rate für diesen Provider\n",
    "        anomaly_rate = 0\n",
    "        if anomalies_df is not None and len(anomalies_df) > 0:\n",
    "            provider_anomalies = len(anomalies_df[anomalies_df['provider'] == provider])\n",
    "            # Approximiere Gesamtmessungen\n",
    "            provider_measurements = len([p for p in network_paths if p['provider'] == provider])\n",
    "            anomaly_rate = (provider_anomalies / provider_measurements * 100) if provider_measurements > 0 else 0\n",
    "        \n",
    "        comparison_matrix.append({\n",
    "            'Provider': provider,\n",
    "            'Total_Edges': total_edges,\n",
    "            'Regions': regions,\n",
    "            'Avg_Edges_Per_Region': avg_edges,\n",
    "            'Geo_Coverage': geo_coverage,\n",
    "            'Anomaly_Rate_%': anomaly_rate\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_matrix)\n",
    "    print(comparison_df.round(2))\n",
    "    \n",
    "    # Infrastruktur-Effizienz-Score\n",
    "    print(f\"\\n⭐ INFRASTRUKTUR-EFFIZIENZ-RANKING:\")\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        # Score basierend auf Edges, Abdeckung und niedrige Anomalien\n",
    "        edge_score = min(row['Total_Edges'] / 10, 10)  # Max 10 Punkte\n",
    "        coverage_score = min(row['Geo_Coverage'] / 2, 10)  # Max 10 Punkte\n",
    "        stability_score = max(0, 10 - row['Anomaly_Rate_%'] / 2)  # Max 10 Punkte\n",
    "        \n",
    "        total_score = (edge_score + coverage_score + stability_score) / 3\n",
    "        \n",
    "        print(f\"  {row['Provider']}: {total_score:.1f}/10\")\n",
    "        print(f\"    Edge-Density: {edge_score:.1f}/10\")\n",
    "        print(f\"    Geo-Coverage: {coverage_score:.1f}/10\")\n",
    "        print(f\"    Stabilität: {stability_score:.1f}/10\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# ================================================================\n",
    "# 4. STATISTISCHE & PRÄDIKTIVE ANALYSEN\n",
    "# ================================================================\n",
    "\n",
    "def statistical_predictive_analysis(perf_df, protocol_name):\n",
    "    \"\"\"Statistische Korrelationen und prädiktive Modellierung\"\"\"\n",
    "    print(f\"\\n4. STATISTISCHE & PRÄDIKTIVE ANALYSEN - {protocol_name}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    if perf_df is None or len(perf_df) == 0:\n",
    "        print(\"Keine Performance-Daten verfügbar\")\n",
    "        return\n",
    "    \n",
    "    # Numerische Korrelationsanalyse\n",
    "    print(f\"\\n📈 KORRELATIONSANALYSE:\")\n",
    "    \n",
    "    numeric_cols = ['latency', 'packet_loss', 'jitter', 'intermediate_failures', 'total_hops']\n",
    "    available_cols = [col for col in numeric_cols if col in perf_df.columns]\n",
    "    \n",
    "    if len(available_cols) >= 2:\n",
    "        correlation_matrix = perf_df[available_cols].corr()\n",
    "        \n",
    "        print(f\"Korrelations-Matrix:\")\n",
    "        print(correlation_matrix.round(3))\n",
    "        \n",
    "        # Signifikante Korrelationen identifizieren\n",
    "        print(f\"\\n🔍 SIGNIFIKANTE KORRELATIONEN (|r| > 0.3):\")\n",
    "        \n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr) > 0.3:\n",
    "                    var1 = correlation_matrix.columns[i]\n",
    "                    var2 = correlation_matrix.columns[j]\n",
    "                    print(f\"  {var1} ↔ {var2}: {corr:.3f}\")\n",
    "    \n",
    "    # Time-Series-Clustering\n",
    "    print(f\"\\n🕰️ TIME-SERIES-CLUSTERING:\")\n",
    "    \n",
    "    anycast_data = perf_df[perf_df['service_type'] == 'anycast'].copy()\n",
    "    \n",
    "    if len(anycast_data) > 0:\n",
    "        # Erstelle Provider-Zeit-Features\n",
    "        anycast_data['hour'] = anycast_data['timestamp'].dt.hour\n",
    "        anycast_data['day_of_week'] = anycast_data['timestamp'].dt.dayofweek\n",
    "        anycast_data['minute_of_day'] = anycast_data['hour'] * 60 + anycast_data['timestamp'].dt.minute\n",
    "        \n",
    "        # Performance-Profile pro Provider\n",
    "        provider_profiles = []\n",
    "        \n",
    "        for provider in anycast_data['provider'].unique():\n",
    "            provider_data = anycast_data[anycast_data['provider'] == provider]\n",
    "            \n",
    "            if len(provider_data) > 100:\n",
    "                # Temporale Features\n",
    "                hourly_avg = provider_data.groupby('hour')['latency'].mean()\n",
    "                daily_avg = provider_data.groupby('day_of_week')['latency'].mean()\n",
    "                \n",
    "                profile = {\n",
    "                    'provider': provider,\n",
    "                    'avg_latency': provider_data['latency'].mean(),\n",
    "                    'latency_std': provider_data['latency'].std(),\n",
    "                    'avg_loss': provider_data['packet_loss'].mean(),\n",
    "                    'avg_jitter': provider_data['jitter'].mean(),\n",
    "                    'peak_hour_variance': hourly_avg.max() - hourly_avg.min(),\n",
    "                    'weekend_weekday_diff': daily_avg[[5, 6]].mean() - daily_avg[[0, 1, 2, 3, 4]].mean()\n",
    "                }\n",
    "                \n",
    "                provider_profiles.append(profile)\n",
    "        \n",
    "        if provider_profiles:\n",
    "            profiles_df = pd.DataFrame(provider_profiles)\n",
    "            \n",
    "            # K-Means Clustering der Provider-Profile\n",
    "            feature_cols = [col for col in profiles_df.columns if col != 'provider']\n",
    "            scaler = StandardScaler()\n",
    "            scaled_features = scaler.fit_transform(profiles_df[feature_cols])\n",
    "            \n",
    "            # Optimale Cluster-Anzahl bestimmen\n",
    "            n_clusters = min(3, len(profiles_df))\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            clusters = kmeans.fit_predict(scaled_features)\n",
    "            \n",
    "            profiles_df['cluster'] = clusters\n",
    "            \n",
    "            print(f\"Provider-Performance-Cluster:\")\n",
    "            for cluster_id in range(n_clusters):\n",
    "                cluster_providers = profiles_df[profiles_df['cluster'] == cluster_id]['provider'].tolist()\n",
    "                print(f\"  Cluster {cluster_id}: {cluster_providers}\")\n",
    "                \n",
    "                cluster_data = profiles_df[profiles_df['cluster'] == cluster_id]\n",
    "                print(f\"    Durchschn. Latenz: {cluster_data['avg_latency'].mean():.2f}ms\")\n",
    "                print(f\"    Durchschn. Stabilität: {cluster_data['latency_std'].mean():.2f}ms\")\n",
    "    \n",
    "    # Predictive Modeling\n",
    "    print(f\"\\n🔮 PREDICTIVE MODELING:\")\n",
    "    \n",
    "    try:\n",
    "        # Einfaches Latenz-Vorhersage-Modell\n",
    "        modeling_data = perf_df.copy()\n",
    "        \n",
    "        # Sichere Zeitstempel-Verarbeitung\n",
    "        if 'hour' not in modeling_data.columns:\n",
    "            modeling_data['timestamp'] = pd.to_datetime(modeling_data['timestamp'])\n",
    "            modeling_data['hour'] = modeling_data['timestamp'].dt.hour\n",
    "            modeling_data['day_of_week_num'] = modeling_data['timestamp'].dt.dayofweek\n",
    "        else:\n",
    "            # Falls hour schon existiert, erstelle day_of_week_num\n",
    "            if 'day_of_week_num' not in modeling_data.columns:\n",
    "                modeling_data['timestamp'] = pd.to_datetime(modeling_data['timestamp'])\n",
    "                modeling_data['day_of_week_num'] = modeling_data['timestamp'].dt.dayofweek\n",
    "        \n",
    "        # Features für Modellierung\n",
    "        feature_columns = ['hour', 'day_of_week_num', 'total_hops']\n",
    "        \n",
    "        # Kategorische Variablen zu numerisch\n",
    "        service_type_mapping = {'anycast': 0, 'pseudo-anycast': 1, 'unicast': 2}\n",
    "        modeling_data['service_type_numeric'] = modeling_data['service_type'].map(service_type_mapping)\n",
    "        feature_columns.append('service_type_numeric')\n",
    "        \n",
    "        # Entferne Zeilen mit fehlenden Werten\n",
    "        modeling_data = modeling_data.dropna(subset=feature_columns + ['latency'])\n",
    "        \n",
    "        if len(modeling_data) > 1000:\n",
    "            try:\n",
    "                from sklearn.ensemble import RandomForestRegressor\n",
    "                from sklearn.model_selection import train_test_split\n",
    "                from sklearn.metrics import mean_squared_error, r2_score\n",
    "                \n",
    "                X = modeling_data[feature_columns]\n",
    "                y = modeling_data['latency']\n",
    "                \n",
    "                # Train-Test Split\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                \n",
    "                # Random Forest Modell\n",
    "                rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                rf_model.fit(X_train, y_train)\n",
    "                \n",
    "                # Vorhersagen\n",
    "                y_pred = rf_model.predict(X_test)\n",
    "                \n",
    "                # Modell-Evaluation\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                print(f\"  Latenz-Vorhersage-Modell:\")\n",
    "                print(f\"    R² Score: {r2:.3f}\")\n",
    "                print(f\"    RMSE: {np.sqrt(mse):.3f}ms\")\n",
    "                \n",
    "                # Feature-Wichtigkeit\n",
    "                feature_importance = rf_model.feature_importances_\n",
    "                print(f\"    Feature-Wichtigkeit:\")\n",
    "                for feat, imp in zip(feature_columns, feature_importance):\n",
    "                    print(f\"      {feat}: {imp:.3f}\")\n",
    "                \n",
    "                # Performance-Kategorien vorhersagen\n",
    "                print(f\"\\n  Performance-Kategorie-Vorhersage:\")\n",
    "                \n",
    "                # Kategorisiere Latenz\n",
    "                latency_categories = pd.cut(modeling_data['latency'], \n",
    "                                          bins=[0, 5, 20, 100, float('inf')], \n",
    "                                          labels=['Excellent', 'Good', 'Fair', 'Poor'])\n",
    "                \n",
    "                modeling_data['latency_category'] = latency_categories\n",
    "                \n",
    "                # Häufigkeitsverteilung\n",
    "                category_dist = modeling_data['latency_category'].value_counts()\n",
    "                print(f\"    Latenz-Kategorien:\")\n",
    "                for category, count in category_dist.items():\n",
    "                    print(f\"      {category}: {count:,} ({count/len(modeling_data)*100:.1f}%)\")\n",
    "                \n",
    "            except ImportError:\n",
    "                print(f\"  ⚠️ Scikit-learn nicht verfügbar - überspringe Machine Learning\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Predictive Modeling fehlgeschlagen: {e}\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ Nicht genügend Daten für Predictive Modeling ({len(modeling_data)} < 1000)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Predictive Modeling komplett fehlgeschlagen: {e}\")\n",
    "        print(f\"  Verfügbare Spalten: {list(perf_df.columns) if perf_df is not None else 'Keine'}\")\n",
    "\n",
    "# ================================================================\n",
    "# 5. QUALITÄTS- UND SLA-ANALYSEN\n",
    "# ================================================================\n",
    "\n",
    "def quality_sla_analysis(perf_df, protocol_name):\n",
    "    \"\"\"Service-Quality und SLA-Compliance-Analyse\"\"\"\n",
    "    print(f\"\\n5. QUALITÄTS- UND SLA-ANALYSEN - {protocol_name}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if perf_df is None or len(perf_df) == 0:\n",
    "        print(\"Keine Performance-Daten verfügbar\")\n",
    "        return\n",
    "    \n",
    "    # Standard SLA-Schwellwerte definieren\n",
    "    SLA_THRESHOLDS = {\n",
    "        'latency_excellent': 5,      # < 5ms\n",
    "        'latency_good': 20,          # < 20ms\n",
    "        'latency_acceptable': 100,    # < 100ms\n",
    "        'packet_loss_max': 1,        # < 1%\n",
    "        'availability_min': 99.9     # > 99.9%\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n📊 SLA-COMPLIANCE-ANALYSE:\")\n",
    "    print(f\"SLA-Schwellwerte:\")\n",
    "    for threshold, value in SLA_THRESHOLDS.items():\n",
    "        print(f\"  {threshold}: {value}\")\n",
    "    \n",
    "    # Service-Level-Analyse\n",
    "    sla_results = []\n",
    "    \n",
    "    for service_type in perf_df['service_type'].unique():\n",
    "        type_data = perf_df[perf_df['service_type'] == service_type]\n",
    "        \n",
    "        if len(type_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Latenz-SLA-Compliance\n",
    "        excellent_latency = (type_data['latency'] < SLA_THRESHOLDS['latency_excellent']).mean() * 100\n",
    "        good_latency = (type_data['latency'] < SLA_THRESHOLDS['latency_good']).mean() * 100\n",
    "        acceptable_latency = (type_data['latency'] < SLA_THRESHOLDS['latency_acceptable']).mean() * 100\n",
    "        \n",
    "        # Packet-Loss-SLA\n",
    "        low_loss = (type_data['packet_loss'] < SLA_THRESHOLDS['packet_loss_max']).mean() * 100\n",
    "        \n",
    "        # Availability (basierend auf erfolgreichen Messungen)\n",
    "        availability = ((type_data['packet_loss'] < 100).mean() * 100)\n",
    "        \n",
    "        sla_result = {\n",
    "            'Service_Type': service_type,\n",
    "            'Excellent_Latency_%': excellent_latency,\n",
    "            'Good_Latency_%': good_latency,\n",
    "            'Acceptable_Latency_%': acceptable_latency,\n",
    "            'Low_PacketLoss_%': low_loss,\n",
    "            'Availability_%': availability\n",
    "        }\n",
    "        \n",
    "        sla_results.append(sla_result)\n",
    "    \n",
    "    sla_df = pd.DataFrame(sla_results)\n",
    "    print(f\"\\nSLA-Compliance-Übersicht:\")\n",
    "    print(sla_df.round(1))\n",
    "    \n",
    "    # Provider-spezifische SLA-Analyse\n",
    "    print(f\"\\n🏢 PROVIDER-SLA-SCORECARD:\")\n",
    "    \n",
    "    anycast_data = perf_df[perf_df['service_type'] == 'anycast']\n",
    "    \n",
    "    for provider in anycast_data['provider'].unique():\n",
    "        provider_data = anycast_data[anycast_data['provider'] == provider]\n",
    "        \n",
    "        if len(provider_data) > 0:\n",
    "            # SLA-Metriken berechnen\n",
    "            excellent_rate = (provider_data['latency'] < 5).mean() * 100\n",
    "            availability = (provider_data['packet_loss'] < 100).mean() * 100\n",
    "            reliability = (provider_data['packet_loss'] < 1).mean() * 100\n",
    "            \n",
    "            # Worst-Case-Szenarien\n",
    "            p95_latency = provider_data['latency'].quantile(0.95)\n",
    "            p99_latency = provider_data['latency'].quantile(0.99)\n",
    "            max_latency = provider_data['latency'].max()\n",
    "            \n",
    "            print(f\"\\n  {provider}:\")\n",
    "            print(f\"    Excellent Performance: {excellent_rate:.1f}%\")\n",
    "            print(f\"    Availability: {availability:.1f}%\")\n",
    "            print(f\"    Reliability: {reliability:.1f}%\")\n",
    "            print(f\"    95th Percentile Latenz: {p95_latency:.1f}ms\")\n",
    "            print(f\"    99th Percentile Latenz: {p99_latency:.1f}ms\")\n",
    "            print(f\"    Worst-Case Latenz: {max_latency:.1f}ms\")\n",
    "            \n",
    "            # Overall SLA-Score\n",
    "            sla_score = (excellent_rate * 0.4 + availability * 0.3 + reliability * 0.3)\n",
    "            print(f\"    📋 Overall SLA-Score: {sla_score:.1f}/100\")\n",
    "    \n",
    "    # Performance-Degradation-Analyse\n",
    "    print(f\"\\n📉 PERFORMANCE-DEGRADATION-ANALYSE:\")\n",
    "    \n",
    "    # Sichere Zeitstempel-Verarbeitung\n",
    "    try:\n",
    "        if 'hour' not in perf_df.columns:\n",
    "            perf_df['timestamp'] = pd.to_datetime(perf_df['timestamp'])\n",
    "            perf_df['hour'] = perf_df['timestamp'].dt.hour\n",
    "        \n",
    "        if 'day_of_week' not in perf_df.columns:\n",
    "            perf_df['day_of_week'] = perf_df['timestamp'].dt.day_name()\n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️ Zeitstempel-Verarbeitung fehlgeschlagen: {e}\")\n",
    "        print(f\"    Überspringe Performance-Degradation-Analyse\")\n",
    "        return sla_df\n",
    "    \n",
    "    # Peak vs. Off-Peak für Anycast\n",
    "    if len(anycast_data) > 0:\n",
    "        try:\n",
    "            # Business Hours (9-17 UTC) vs. Off-Hours\n",
    "            anycast_with_time = anycast_data.copy()\n",
    "            if 'hour' not in anycast_with_time.columns:\n",
    "                anycast_with_time['timestamp'] = pd.to_datetime(anycast_with_time['timestamp'])\n",
    "                anycast_with_time['hour'] = anycast_with_time['timestamp'].dt.hour\n",
    "                anycast_with_time['day_of_week'] = anycast_with_time['timestamp'].dt.day_name()\n",
    "            \n",
    "            business_hours = anycast_with_time[anycast_with_time['hour'].between(9, 17)]\n",
    "            off_hours = anycast_with_time[~anycast_with_time['hour'].between(9, 17)]\n",
    "            \n",
    "            if len(business_hours) > 0 and len(off_hours) > 0:\n",
    "                business_latency = business_hours['latency'].mean()\n",
    "                off_hours_latency = off_hours['latency'].mean()\n",
    "                degradation = ((business_latency - off_hours_latency) / off_hours_latency) * 100\n",
    "                \n",
    "                print(f\"  Business Hours vs. Off-Hours:\")\n",
    "                print(f\"    Business Hours Latenz: {business_latency:.1f}ms\")\n",
    "                print(f\"    Off-Hours Latenz: {off_hours_latency:.1f}ms\")\n",
    "                print(f\"    Performance-Degradation: {degradation:.1f}%\")\n",
    "            \n",
    "            # Wochenende vs. Werktage\n",
    "            weekend_data = anycast_with_time[anycast_with_time['day_of_week'].isin(['Saturday', 'Sunday'])]\n",
    "            weekday_data = anycast_with_time[~anycast_with_time['day_of_week'].isin(['Saturday', 'Sunday'])]\n",
    "            \n",
    "            if len(weekend_data) > 0 and len(weekday_data) > 0:\n",
    "                weekend_latency = weekend_data['latency'].mean()\n",
    "                weekday_latency = weekday_data['latency'].mean()\n",
    "                weekend_effect = ((weekday_latency - weekend_latency) / weekend_latency) * 100\n",
    "                \n",
    "                print(f\"  Wochenende vs. Werktage:\")\n",
    "                print(f\"    Wochenende Latenz: {weekend_latency:.1f}ms\")\n",
    "                print(f\"    Werktage Latenz: {weekday_latency:.1f}ms\")\n",
    "                print(f\"    Wochenend-Effekt: {weekend_effect:.1f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ Performance-Degradation-Analyse fehlgeschlagen: {e}\")\n",
    "            print(f\"    Verfügbare Spalten: {list(anycast_data.columns)}\")\n",
    "    else:\n",
    "        print(f\"    Keine Anycast-Daten verfügbar\")\n",
    "    \n",
    "    return sla_df\n",
    "\n",
    "# ================================================================\n",
    "# 6. AKAMAI-PROBLEM DEEP-DIVE\n",
    "# ================================================================\n",
    "\n",
    "def akamai_deep_dive_analysis(network_paths, perf_df, protocol_name):\n",
    "    \"\"\"Detaillierte Analyse des Akamai Pseudo-Anycast Problems\"\"\"\n",
    "    print(f\"\\n6. AKAMAI-PROBLEM DEEP-DIVE - {protocol_name}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if not network_paths or perf_df is None:\n",
    "        print(\"Keine Daten für Akamai-Analyse verfügbar\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🔍 AKAMAI vs. ECHTE ANYCAST ARCHITEKTUR-VERGLEICH:\")\n",
    "    \n",
    "    # Akamai-Pfade vs. echte Anycast-Pfade\n",
    "    akamai_paths = [p for p in network_paths if p['provider'] == 'Akamai']\n",
    "    cloudflare_paths = [p for p in network_paths if p['provider'] == 'Cloudflare']\n",
    "    google_paths = [p for p in network_paths if p['provider'] == 'Google']\n",
    "    \n",
    "    print(f\"\\nRouting-Diversität-Vergleich:\")\n",
    "    print(f\"  Akamai Pfade: {len(akamai_paths)}\")\n",
    "    print(f\"  Cloudflare Pfade: {len(cloudflare_paths)}\")\n",
    "    print(f\"  Google Pfade: {len(google_paths)}\")\n",
    "    \n",
    "    # ASN-Diversitäts-Analyse\n",
    "    def analyze_asn_diversity(paths, provider_name):\n",
    "        if not paths:\n",
    "            return\n",
    "        \n",
    "        all_asns = set()\n",
    "        region_asns = defaultdict(set)\n",
    "        final_destinations = defaultdict(set)\n",
    "        \n",
    "        for path in paths:\n",
    "            region = path['region']\n",
    "            for asn in path['asns']:\n",
    "                all_asns.add(asn)\n",
    "                region_asns[region].add(asn)\n",
    "            \n",
    "            # Finale Destination\n",
    "            if path['hops']:\n",
    "                final_hop = path['hops'][-1]['hostname']\n",
    "                final_destinations[region].add(final_hop)\n",
    "        \n",
    "        print(f\"\\n  {provider_name}:\")\n",
    "        print(f\"    Gesamte ASNs: {len(all_asns)}\")\n",
    "        print(f\"    Durchschn. ASNs/Region: {np.mean([len(asns) for asns in region_asns.values()]):.1f}\")\n",
    "        \n",
    "        # Finale Destination-Diversität\n",
    "        total_destinations = sum(len(dests) for dests in final_destinations.values())\n",
    "        print(f\"    Finale Destinations: {total_destinations}\")\n",
    "        print(f\"    Durchschn. Destinations/Region: {total_destinations/len(final_destinations):.1f}\")\n",
    "        \n",
    "        # Anycast-Beweis: Verschiedene ASNs aber gleiche finale IPs\n",
    "        unique_final_ips = set()\n",
    "        for path in paths:\n",
    "            if path['hops']:\n",
    "                # Extrahiere IP aus Hostname falls möglich\n",
    "                hostname = path['hops'][-1]['hostname']\n",
    "                if '(' in hostname and ')' in hostname:\n",
    "                    ip = hostname.split('(')[-1].split(')')[0]\n",
    "                    unique_final_ips.add(ip)\n",
    "        \n",
    "        print(f\"    Eindeutige finale IPs: {len(unique_final_ips)}\")\n",
    "        \n",
    "        return {\n",
    "            'total_asns': len(all_asns),\n",
    "            'avg_asns_per_region': np.mean([len(asns) for asns in region_asns.values()]),\n",
    "            'total_destinations': total_destinations,\n",
    "            'unique_final_ips': len(unique_final_ips)\n",
    "        }\n",
    "    \n",
    "    # Vergleiche Provider\n",
    "    akamai_stats = analyze_asn_diversity(akamai_paths, \"Akamai\")\n",
    "    cloudflare_stats = analyze_asn_diversity(cloudflare_paths, \"Cloudflare\")\n",
    "    google_stats = analyze_asn_diversity(google_paths, \"Google\")\n",
    "    \n",
    "    # Performance-Vergleich\n",
    "    print(f\"\\n📊 PERFORMANCE-ARCHITEKTUR-KORRELATION:\")\n",
    "    \n",
    "    akamai_perf = perf_df[perf_df['provider'] == 'Akamai']\n",
    "    cloudflare_perf = perf_df[perf_df['provider'] == 'Cloudflare']\n",
    "    google_perf = perf_df[perf_df['provider'] == 'Google']\n",
    "    \n",
    "    performance_comparison = []\n",
    "    \n",
    "    for provider, data in [('Akamai', akamai_perf), ('Cloudflare', cloudflare_perf), ('Google', google_perf)]:\n",
    "        if len(data) > 0:\n",
    "            perf_metrics = {\n",
    "                'Provider': provider,\n",
    "                'Avg_Latency': data['latency'].mean(),\n",
    "                'Latency_Std': data['latency'].std(),\n",
    "                'P95_Latency': data['latency'].quantile(0.95),\n",
    "                'Avg_PacketLoss': data['packet_loss'].mean(),\n",
    "                'Availability': (data['packet_loss'] < 100).mean() * 100\n",
    "            }\n",
    "            performance_comparison.append(perf_metrics)\n",
    "    \n",
    "    perf_comparison_df = pd.DataFrame(performance_comparison)\n",
    "    print(perf_comparison_df.round(2))\n",
    "    \n",
    "    # Akamai-Problem-Diagnose\n",
    "    print(f\"\\n🚨 AKAMAI-PROBLEM-DIAGNOSE:\")\n",
    "    \n",
    "    if akamai_stats and cloudflare_stats:\n",
    "        print(f\"\\n  Routing-Diversität-Defizit:\")\n",
    "        asn_deficit = (cloudflare_stats['total_asns'] - akamai_stats['total_asns']) / cloudflare_stats['total_asns'] * 100\n",
    "        print(f\"    ASN-Diversität-Defizit: {asn_deficit:.1f}% weniger als Cloudflare\")\n",
    "        \n",
    "        if akamai_stats['unique_final_ips'] == 1:\n",
    "            print(f\"    🔴 PROBLEM: Akamai routet zu nur 1 finaler IP (echtes Unicast)\")\n",
    "        else:\n",
    "            print(f\"    🟡 WARNUNG: Akamai hat {akamai_stats['unique_final_ips']} finale IPs\")\n",
    "    \n",
    "    # Regionale Akamai-Ineffizienz\n",
    "    print(f\"\\n🌍 REGIONALE AKAMAI-INEFFIZIENZ:\")\n",
    "    \n",
    "    if len(akamai_perf) > 0:\n",
    "        regional_akamai = akamai_perf.groupby('region')['latency'].agg(['mean', 'std']).round(1)\n",
    "        regional_akamai = regional_akamai.sort_values('mean', ascending=False)\n",
    "        \n",
    "        print(f\"Schlechteste Akamai-Regionen:\")\n",
    "        for region, stats in regional_akamai.head(5).iterrows():\n",
    "            print(f\"  {region}: {stats['mean']:.1f}ms (±{stats['std']:.1f}ms)\")\n",
    "    \n",
    "    # Akamai vs. geografische Referenz\n",
    "    print(f\"\\n📍 AKAMAI vs. GEOGRAFISCHE REFERENZ:\")\n",
    "    \n",
    "    unicast_perf = perf_df[perf_df['service_type'] == 'unicast']\n",
    "    \n",
    "    if len(akamai_perf) > 0 and len(unicast_perf) > 0:\n",
    "        akamai_avg = akamai_perf['latency'].mean()\n",
    "        unicast_avg = unicast_perf['latency'].mean()\n",
    "        \n",
    "        performance_ratio = akamai_avg / unicast_avg\n",
    "        \n",
    "        print(f\"  Akamai Durchschn. Latenz: {akamai_avg:.1f}ms\")\n",
    "        print(f\"  Unicast Durchschn. Latenz: {unicast_avg:.1f}ms\")\n",
    "        print(f\"  Performance-Ratio: {performance_ratio:.2f}\")\n",
    "        \n",
    "        if performance_ratio > 0.8:\n",
    "            print(f\"  🔴 BESTÄTIGT: Akamai verhält sich wie Unicast ({performance_ratio:.2f}x)\")\n",
    "        else:\n",
    "            print(f\"  🟡 TEILWEISE: Akamai besser als Unicast, aber schlechter als echtes Anycast\")\n",
    "\n",
    "# ================================================================\n",
    "# 7. HAUPTANALYSE-FUNKTION - ALLE ERWEITERTEN ANALYSEN\n",
    "# ================================================================\n",
    "\n",
    "def run_comprehensive_analysis():\n",
    "    \"\"\"Führt alle erweiterten Analysen durch\"\"\"\n",
    "    \n",
    "    # WICHTIG: Passen Sie diese Pfade an Ihre Parquet-Files an!\n",
    "    IPv4_FILE = \"../data/IPv4.parquet\"  # Bitte anpassen\n",
    "    IPv6_FILE = \"../data/IPv6.parquet\"  # Bitte anpassen\n",
    "    \n",
    "    # Alternativen für häufige Dateipfade:\n",
    "    # IPv4_FILE = \"ipv4_measurements.parquet\"\n",
    "    # IPv4_FILE = \"data/ipv4_data.parquet\" \n",
    "    # IPv4_FILE = \"/full/path/to/ipv4_data.parquet\"\n",
    "    \n",
    "    print(\"🔄 LADE DATEN FÜR UMFASSENDE ANALYSE...\")\n",
    "    print(f\"Versuche IPv4-Datei zu laden: {IPv4_FILE}\")\n",
    "    print(f\"Versuche IPv6-Datei zu laden: {IPv6_FILE}\")\n",
    "    \n",
    "    try:\n",
    "        df_ipv4 = pd.read_parquet(IPv4_FILE)\n",
    "        print(f\"✅ IPv4: {df_ipv4.shape[0]:,} Messungen erfolgreich geladen\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ IPv4-Datei nicht gefunden: {IPv4_FILE}\")\n",
    "        print(\"💡 LÖSUNG: Passen Sie IPv4_FILE in der Funktion run_comprehensive_analysis() an\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fehler beim Laden der IPv4-Daten: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df_ipv6 = pd.read_parquet(IPv6_FILE)\n",
    "        print(f\"✅ IPv6: {df_ipv6.shape[0]:,} Messungen erfolgreich geladen\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ IPv6-Datei nicht gefunden: {IPv6_FILE}\")\n",
    "        print(\"💡 LÖSUNG: Passen Sie IPv6_FILE in der Funktion run_comprehensive_analysis() an\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fehler beim Laden der IPv6-Daten: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🚀 BEIDE DATEIEN ERFOLGREICH GELADEN - STARTE UMFASSENDE ANALYSE...\")\n",
    "    \n",
    "    try:\n",
    "        # Führe alle Analysen für beide Protokolle durch\n",
    "        for protocol, df in [(\"IPv4\", df_ipv4), (\"IPv6\", df_ipv6)]:\n",
    "            print(f\"\\n{'='*85}\")\n",
    "            print(f\"UMFASSENDE ANALYSE FÜR {protocol}\")\n",
    "            print(f\"{'='*85}\")\n",
    "            \n",
    "            try:\n",
    "                # 1. Netzwerk-Topologie\n",
    "                print(f\"🌐 Starte Netzwerk-Topologie-Analyse...\")\n",
    "                network_paths, asn_analysis = analyze_network_topology(df, protocol)\n",
    "                \n",
    "                # 2. Anomalie-Deep-Dive\n",
    "                print(f\"🚨 Starte Anomalie-Deep-Dive...\")\n",
    "                anomalies_df, perf_df = comprehensive_anomaly_analysis(df, protocol)\n",
    "                \n",
    "                # 3. Provider-Infrastruktur-Mapping\n",
    "                print(f\"🏗️ Starte Provider-Infrastruktur-Mapping...\")\n",
    "                infrastructure_comparison = map_provider_infrastructure(network_paths, anomalies_df, protocol)\n",
    "                \n",
    "                # 4. Statistische & Prädiktive Analysen\n",
    "                print(f\"📊 Starte Statistische & Prädiktive Analysen...\")\n",
    "                statistical_predictive_analysis(perf_df, protocol)\n",
    "                \n",
    "                # 5. Qualitäts- und SLA-Analysen\n",
    "                print(f\"📋 Starte Qualitäts- und SLA-Analysen...\")\n",
    "                sla_results = quality_sla_analysis(perf_df, protocol)\n",
    "                \n",
    "                # 6. Akamai-Problem Deep-Dive\n",
    "                print(f\"🔍 Starte Akamai-Problem Deep-Dive...\")\n",
    "                akamai_deep_dive_analysis(network_paths, perf_df, protocol)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Fehler in {protocol}-Analyse: {e}\")\n",
    "                print(f\"Setze mit nächstem Protokoll fort...\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n{'='*85}\")\n",
    "        print(\"🎯 ALLE ERWEITERTEN ANALYSEN ABGESCHLOSSEN!\")\n",
    "        print(\"🏆 VOLLSTÄNDIGE ANYCAST-FORSCHUNGSSTUDIE ERSTELLT!\")\n",
    "        print(\"=\"*85)\n",
    "        \n",
    "        print(f\"\\n📋 KOMPLETTE ANALYSE-ÜBERSICHT:\")\n",
    "        analysis_components = [\n",
    "            \"✅ Phase 1: Datenverständnis & Überblick\",\n",
    "            \"✅ Phase 2: Geografische Routing-Analyse\", \n",
    "            \"✅ Phase 3: Performance-Trends & Zeitanalyse\",\n",
    "            \"✅ Phase 4a: Netzwerk-Topologie & Infrastruktur\",\n",
    "            \"✅ Phase 4b: Anomalie-Deep-Dive & Klassifikation\",\n",
    "            \"✅ Phase 4c: Provider-Infrastruktur-Mapping\",\n",
    "            \"✅ Phase 4d: Statistische & Prädiktive Analysen\",\n",
    "            \"✅ Phase 4e: Qualitäts- & SLA-Analysen\",\n",
    "            \"✅ Phase 4f: Akamai-Problem Deep-Dive\"\n",
    "        ]\n",
    "        \n",
    "        for component in analysis_components:\n",
    "            print(component)\n",
    "        \n",
    "        print(f\"\\n🚀 BEREIT FÜR:\")\n",
    "        print(\"  • Wissenschaftliche Publikation\")\n",
    "        print(\"  • Konferenz-Präsentation\")\n",
    "        print(\"  • Industry-Report\")\n",
    "        print(\"  • PhD-Dissertation-Kapitel\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unerwarteter Fehler in der Hauptanalyse: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ANWEISUNGEN ZUM AUSFÜHREN:\n",
    "print(\"=\"*85)\n",
    "print(\"📋 ANWEISUNGEN FÜR PHASE 4:\")\n",
    "print(\"=\"*85)\n",
    "print(\"1. Passen Sie die Dateipfade IPv4_FILE und IPv6_FILE an (Zeile ~970-971)\")\n",
    "print(\"2. Führen Sie run_comprehensive_analysis() aus\")\n",
    "print(\"3. Die Analyse dauert mehrere Minuten - seien Sie geduldig!\")\n",
    "print(\"4. Alle Ergebnisse werden in der Konsole ausgegeben\")\n",
    "print(\"=\"*85)\n",
    "\n",
    "# Führe die umfassende Analyse aus\n",
    "if __name__ == \"__main__\":\n",
    "    run_comprehensive_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
